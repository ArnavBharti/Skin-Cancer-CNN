{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMDuhzfMxd5heaHw+uuHYlt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArnavBharti/Skin-Cancer-CNN/blob/main/skin_cancer_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Skin Cancer Classification with MobileNet\n",
        "\n",
        "This notebook contains the complete pipeline for classifying 7 types of skin lesions using two different MobileNet-based architectures.\n",
        "\n",
        "**The process includes:**\n",
        "1.  **Setup**: Mounting Google Drive and importing libraries.\n",
        "2.  **Configuration**: Setting up paths, hyperparameters, and class weights.\n",
        "3.  **Data Loading**: Creating a custom PyTorch Dataset and DataLoader.\n",
        "4.  **Model Definition**: Defining two MobileNet-based models.\n",
        "5.  **Training & Evaluation**: Training the models, saving the best weights, and evaluating performance.\n",
        "6.  **Results**: Visualizing training history, confusion matrices, and detailed classification reports."
      ],
      "metadata": {
        "id": "pEEeGNdDLjIV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SECTION 1: SETUP AND CONFIGURATION"
      ],
      "metadata": {
        "id": "yLgVvYISL2Iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "print(\"Libraries imported successfully.\")\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(f\"Using device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
      ],
      "metadata": {
        "id": "vj7XhZJ_L-2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Paths ---\n",
        "BASE_DRIVE_PATH = \"/content/drive/My Drive/dl_project/\"\n",
        "TRAIN_DIR = os.path.join(BASE_DRIVE_PATH, \"train\")\n",
        "VAL_DIR = os.path.join(BASE_DRIVE_PATH, \"val\")\n",
        "MODEL_SAVE_DIR = os.path.join(BASE_DRIVE_PATH, \"models\")\n",
        "\n",
        "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
        "print(f\"Models will be saved to: {MODEL_SAVE_DIR}\")\n",
        "\n",
        "# --- Device ---\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Model & Training Hyperparameters ---\n",
        "NUM_CLASSES = 7\n",
        "IMAGE_SIZE = 224\n",
        "BATCH_SIZE = 24\n",
        "NUM_EPOCHS = 42\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "# --- Class Definitions ---\n",
        "CLASS_NAMES = {\n",
        "    0: 'akiec', # Actinic keratoses\n",
        "    1: 'bcc',   # Basal cell carcinoma\n",
        "    2: 'bkl',   # Benign keratosis-like lesions\n",
        "    3: 'df',    # Dermatofibroma\n",
        "    4: 'mel',   # Melanoma\n",
        "    5: 'nv',    # Melanocytic nevi\n",
        "    6: 'vasc'   # Vascular lesions\n",
        "}\n",
        "CLASS_IDX_TO_NAME = [CLASS_NAMES[i] for i in range(NUM_CLASSES)]"
      ],
      "metadata": {
        "id": "ZwbLfQxYMTif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SECTION 2: DATA LOADING AND AUGMENTATION"
      ],
      "metadata": {
        "id": "fuOVWNG8Mbe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "N_O7j2llMZXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SkinCancerDataset(Dataset):\n",
        "    \"\"\"Custom PyTorch dataset for loading skin lesion images.\"\"\"\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = [f for f in os.listdir(data_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        if not self.image_files:\n",
        "            raise FileNotFoundError(f\"No images found in directory: {data_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_files[idx]\n",
        "        img_path = os.path.join(self.data_dir, img_name)\n",
        "\n",
        "        try:\n",
        "            # The label is the first number in the filename (e.g., '1_abc.jpg')\n",
        "            label = int(img_name.split('_')[0]) - 1\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "\n",
        "            return image, label\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image or label for {img_path}: {e}\")\n",
        "            # Return a placeholder if an image is corrupt\n",
        "            return torch.randn(3, IMAGE_SIZE, IMAGE_SIZE), 0"
      ],
      "metadata": {
        "id": "qKHwF7dCMgVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    train_labels = [int(f.split('_')[0]) - 1 for f in os.listdir(TRAIN_DIR)]\n",
        "    class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
        "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(DEVICE)\n",
        "    print(\"Calculated Class Weights:\")\n",
        "    for i, weight in enumerate(class_weights):\n",
        "        print(f\"- {CLASS_NAMES[i]}: {weight:.2f}\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not calculate class weights, using uniform weights. Error: {e}\")\n",
        "    class_weights = torch.ones(NUM_CLASSES).to(DEVICE)"
      ],
      "metadata": {
        "id": "MNdGXDHyMljX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SkinCancerDataset(TRAIN_DIR, transform=train_transforms)\n",
        "val_dataset = SkinCancerDataset(VAL_DIR, transform=val_transforms)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "print(f\"\\nDataLoaders created.\")\n",
        "print(f\"Training images: {len(train_dataset)}\")\n",
        "print(f\"Validation images: {len(val_dataset)}\")"
      ],
      "metadata": {
        "id": "7f4XC8goMobK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SECTION 3: MODEL ARCHITECTURES"
      ],
      "metadata": {
        "id": "6uHJEjm5MrHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Mish(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x * torch.tanh(nn.functional.softplus(x))"
      ],
      "metadata": {
        "id": "Btiq1Kp7MpPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MobileNetModel1(nn.Module):\n",
        "    def __init__(self, num_classes=NUM_CLASSES, freeze_layers=False):\n",
        "        super(MobileNetModel1, self).__init__()\n",
        "        self.mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
        "\n",
        "        if freeze_layers:\n",
        "            for param in self.mobilenet.features.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        in_features = self.mobilenet.classifier[1].in_features\n",
        "        self.mobilenet.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(in_features, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mobilenet(x)"
      ],
      "metadata": {
        "id": "E5aP1A6UMwve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MobileNetModel2(nn.Module):\n",
        "    def __init__(self, num_classes=NUM_CLASSES, freeze_layers=False):\n",
        "        super(MobileNetModel2, self).__init__()\n",
        "        self.mobilenet = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
        "\n",
        "        if freeze_layers:\n",
        "            for param in self.mobilenet.features.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        in_features = self.mobilenet.classifier[0].in_features\n",
        "        self.mobilenet.classifier = nn.Sequential(\n",
        "            nn.Linear(in_features, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            Mish(),\n",
        "            nn.Dropout(p=0.4),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mobilenet(x)\n",
        "\n",
        "print(\"Model architectures defined.\")"
      ],
      "metadata": {
        "id": "nk4yWAMXMxwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SECTION 4: UTILITY FUNCTIONS"
      ],
      "metadata": {
        "id": "YD1VQYhfM350"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_history(history, model_name):\n",
        "    \"\"\"Plots training and validation loss and accuracy.\"\"\"\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "    plt.figure(figsize=(16, 6))\n",
        "\n",
        "    # Plot Loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, history['train_loss'], 'bo-', label='Training Loss')\n",
        "    plt.plot(epochs, history['val_loss'], 'ro-', label='Validation Loss')\n",
        "    plt.title(f'{model_name} - Training & Validation Loss', fontsize=14)\n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.ylabel('Loss', fontsize=12)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot Accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, history['train_acc'], 'bo-', label='Training Accuracy')\n",
        "    plt.plot(epochs, history['val_acc'], 'ro-', label='Validation Accuracy')\n",
        "    plt.title(f'{model_name} - Training & Validation Accuracy', fontsize=14)\n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.ylabel('Accuracy', fontsize=12)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(conf_matrix, model_name):\n",
        "    \"\"\"Plots a confusion matrix using Seaborn.\"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=CLASS_IDX_TO_NAME,\n",
        "                yticklabels=CLASS_IDX_TO_NAME)\n",
        "    plt.xlabel('Predicted Label', fontsize=12)\n",
        "    plt.ylabel('True Label', fontsize=12)\n",
        "    plt.title(f'{model_name} - Confusion Matrix (Validation Set)', fontsize=14)\n",
        "    plt.show()\n",
        "\n",
        "print(\"Utility functions defined.\")"
      ],
      "metadata": {
        "id": "UueAJtQkM2SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SECTION 5: TRAINING & EVALUATION LOGIC"
      ],
      "metadata": {
        "id": "ye5i0x-vM-W2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 5.1. Training Function\n",
        "def train_model(model, model_name, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs):\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "    best_val_acc = 0.0\n",
        "    model_save_path = os.path.join(MODEL_SAVE_DIR, f\"{model_name}_best.pth\")\n",
        "\n",
        "    print(f\"\\n--- Starting Training for {model_name} ---\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # --- Training Phase ---\n",
        "        model.train()\n",
        "        running_loss, correct_preds, total_samples = 0.0, 0, 0\n",
        "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
        "\n",
        "        for inputs, labels in train_pbar:\n",
        "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_samples += labels.size(0)\n",
        "            correct_preds += (predicted == labels).sum().item()\n",
        "            train_pbar.set_postfix({'loss': running_loss/total_samples, 'acc': correct_preds/total_samples})\n",
        "\n",
        "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_train_acc = correct_preds / len(train_loader.dataset)\n",
        "        history['train_loss'].append(epoch_train_loss)\n",
        "        history['train_acc'].append(epoch_train_acc)\n",
        "\n",
        "        # --- Validation Phase ---\n",
        "        model.eval()\n",
        "        running_loss, correct_preds, total_samples = 0.0, 0, 0\n",
        "        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_pbar:\n",
        "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total_samples += labels.size(0)\n",
        "                correct_preds += (predicted == labels).sum().item()\n",
        "                val_pbar.set_postfix({'loss': running_loss/total_samples, 'acc': correct_preds/total_samples})\n",
        "\n",
        "        epoch_val_loss = running_loss / len(val_loader.dataset)\n",
        "        epoch_val_acc = correct_preds / len(val_loader.dataset)\n",
        "        history['val_loss'].append(epoch_val_loss)\n",
        "        history['val_acc'].append(epoch_val_acc)\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}: Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f} | Val Loss: {epoch_val_loss:.4f} Acc: {epoch_val_acc:.4f}\")\n",
        "\n",
        "        # Update learning rate\n",
        "        if scheduler:\n",
        "            scheduler.step(epoch_val_loss)\n",
        "\n",
        "        # Save the best model\n",
        "        if epoch_val_acc > best_val_acc:\n",
        "            best_val_acc = epoch_val_acc\n",
        "            torch.save(model.state_dict(), model_save_path)\n",
        "            print(f\"New best validation accuracy: {best_val_acc:.4f}. Model saved to {model_save_path}\")\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\n--- Training Complete for {model_name} ({total_time // 60:.0f}m {total_time % 60:.0f}s) ---\")\n",
        "    print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
        "    return history"
      ],
      "metadata": {
        "id": "GbzmZRciNDP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, data_loader, model_name):\n",
        "    \"\"\"Evaluates the model and prints detailed reports.\"\"\"\n",
        "    model.eval()\n",
        "    all_labels, all_preds = [], []\n",
        "\n",
        "    print(f\"\\n--- Evaluating {model_name} on Validation Set ---\")\n",
        "    eval_pbar = tqdm(data_loader, desc=\"Evaluating\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in eval_pbar:\n",
        "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "    # --- Metrics Calculation ---\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "    class_report = classification_report(\n",
        "        all_labels, all_preds,\n",
        "        target_names=CLASS_IDX_TO_NAME,\n",
        "        zero_division=0\n",
        "    )\n",
        "\n",
        "    # --- Display Results ---\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"               FINAL EVALUATION REPORT: {model_name}\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"\\nFull Classification Report:\")\n",
        "    print(class_report)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plot_confusion_matrix(conf_matrix, model_name)\n"
      ],
      "metadata": {
        "id": "8tp6TnKHNIHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SECTION 6: MODEL 1 - TRAINING AND EVALUATION"
      ],
      "metadata": {
        "id": "sECziDePNK8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1_name = \"MobileNetV2_FineTuned\"\n",
        "model1 = MobileNetModel1().to(DEVICE)\n",
        "criterion1 = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer1 = optim.Adam(model1.parameters(), lr=LEARNING_RATE)\n",
        "scheduler1 = optim.lr_scheduler.ReduceLROnPlateau(optimizer1, mode='min', factor=0.2, patience=3, verbose=True)\n",
        "\n",
        "history1 = train_model(model1, model1_name, train_loader, val_loader, criterion1, optimizer1, scheduler1, num_epochs=NUM_EPOCHS)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HtFXZsB7NOB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_history(history1, model1_name)\n",
        "\n",
        "print(f\"\\n--- Loading best weights for {model1_name} for final evaluation ---\")\n",
        "model1_best_path = os.path.join(MODEL_SAVE_DIR, f\"{model1_name}_best.pth\")\n",
        "try:\n",
        "    model1.load_state_dict(torch.load(model1_best_path))\n",
        "    print(f\"Successfully loaded weights from {model1_best_path}\")\n",
        "    evaluate_model(model1, val_loader, model1_name)\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: Could not find the model file at {model1_best_path}. Evaluation skipped.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the model: {e}\")"
      ],
      "metadata": {
        "id": "_2Pyvai2NO_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SECTION 7: MODEL 2 - TRAINING AND EVALUATION"
      ],
      "metadata": {
        "id": "zKNKyYLCNYTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model2_name = \"MobileNetV3_CustomClassifier\"\n",
        "model2 = MobileNetModel2().to(DEVICE)\n",
        "criterion2 = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer2 = optim.Adam(model2.parameters(), lr=LEARNING_RATE)\n",
        "scheduler2 = optim.lr_scheduler.ReduceLROnPlateau(optimizer2, mode='min', factor=0.2, patience=3, verbose=True)\n",
        "\n",
        "history2 = train_model(model2, model2_name, train_loader, val_loader, criterion2, optimizer2, scheduler2, num_epochs=NUM_EPOCHS)\n"
      ],
      "metadata": {
        "id": "T0fuqcmSNSgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_history(history2, model2_name)\n",
        "\n",
        "print(f\"\\n--- Loading best weights for {model2_name} for final evaluation ---\")\n",
        "model2_best_path = os.path.join(MODEL_SAVE_DIR, f\"{model2_name}_best.pth\")\n",
        "try:\n",
        "    model2.load_state_dict(torch.load(model2_best_path))\n",
        "    print(f\"Successfully loaded weights from {model2_best_path}\")\n",
        "    evaluate_model(model2, val_loader, model2_name)\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: Could not find the model file at {model2_best_path}. Evaluation skipped.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the model: {e}\")"
      ],
      "metadata": {
        "id": "dke57_RSNZU1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}